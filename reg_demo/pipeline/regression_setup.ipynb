{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please wait till this cell completes and then run next cells. This just need to be run once per active kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RELEASE_VERSION=1.0.0\n",
      "Collecting https://storage.googleapis.com/ml-pipeline/release/1.0.0/kfp.tar.gz\n",
      "  Using cached https://storage.googleapis.com/ml-pipeline/release/1.0.0/kfp.tar.gz (122 kB)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (1.31.2)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes<12.0.0,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (11.0.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (1.22.1)\n",
      "Requirement already satisfied, skipping upgrade: requests_toolbelt>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: kfp-server-api<2.0.0,>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: click in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: Deprecated in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (1.2.10)\n",
      "Requirement already satisfied, skipping upgrade: strip-hints in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (0.1.9)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<2.0dev,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==1.0.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==1.0.0) (1.4.3)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==1.0.0) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.24.2 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.3 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (49.6.0.post20201009)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=14.05.14 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.0) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.0) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.0) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==1.0.0) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==1.0.0) (20.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==1.0.0) (0.17.3)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==1.0.0) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==1.0.0) (0.35.1)\n",
      "Requirement already satisfied, skipping upgrade: google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.19.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp==1.0.0) (1.22.4)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==1.0.0) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==1.0.0) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (1.14.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp==1.0.0) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp==1.0.0) (1.52.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp==1.0.0) (3.13.0)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (2.20)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.0.0-py3-none-any.whl size=167373 sha256=8ef6a37dbc7402abc6631544895ec41687ede0c6c61772f595a36fa486270234\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3zjaocxc/wheels/46/ec/3d/7ce52c89bbbc341040f126eaca22d6a02071f01137e5cd5757\n",
      "Successfully built kfp\n",
      "Installing collected packages: kfp\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 1.0.0\n",
      "    Uninstalling kfp-1.0.0:\n",
      "      Successfully uninstalled kfp-1.0.0\n",
      "Successfully installed kfp-1.0.0\n"
     ]
    }
   ],
   "source": [
    "%env RELEASE_VERSION=1.0.0\n",
    "!pip install https://storage.googleapis.com/ml-pipeline/release/${RELEASE_VERSION}/kfp.tar.gz --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List existing pipeline experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiments': [{'created_at': datetime.datetime(2020, 10, 16, 6, 35, 36, tzinfo=tzlocal()),\n",
       "                  'description': 'All runs created without specifying an '\n",
       "                                 'experiment will be grouped here.',\n",
       "                  'id': '2e29d36d-b419-411c-b4de-a731328b865b',\n",
       "                  'name': 'Default',\n",
       "                  'resource_references': None,\n",
       "                  'storage_state': None},\n",
       "                 {'created_at': datetime.datetime(2020, 11, 2, 9, 13, 50, tzinfo=tzlocal()),\n",
       "                  'description': None,\n",
       "                  'id': 'de32226f-6a79-4285-9360-9d26253e57e4',\n",
       "                  'name': 'Dkube - Regression pl',\n",
       "                  'resource_references': None,\n",
       "                  'storage_state': None}],\n",
       " 'next_page_token': None,\n",
       " 'total_size': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dkube REGRESSION experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/de32226f-6a79-4285-9360-9d26253e57e4\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regression_experiment = client.create_experiment(name='Dkube - Regression pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e regression Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "from kfp.components._yaml_utils import load_yaml\n",
    "from kfp.components._yaml_utils import dump_yaml\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "def _component(stage, name):\n",
    "    with open('../components/{}/component.yaml'.format(stage), 'rb') as stream:\n",
    "        cdict = load_yaml(stream)\n",
    "        cdict['name'] = name\n",
    "        cyaml = dump_yaml(cdict)\n",
    "        return components.load_component_from_text(cyaml)\n",
    "        \n",
    "#dkube_preprocess_op         = components.load_component_from_file(\"../components/preprocess/component.yaml\")\n",
    "#dkube_training_op           = components.load_component_from_file(\"../components/training/component.yaml\")\n",
    "#dkube_serving_op            = components.load_component_from_file(\"../components/serving/component.yaml\")\n",
    "#dkube_viewer_op             = components.load_component_from_file('../components/viewer/component.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='dkube-regression-pl',\n",
    "    description='sample regression pipeline with dkube components'\n",
    ")\n",
    "\n",
    "def d3pipeline(\n",
    "    access_url,\n",
    "    user,\n",
    "    auth_token,\n",
    "    #Clinical preprocess\n",
    "    clinical_preprocess_script=\"python cli-pre-processing.py\",\n",
    "    clinical_preprocess_datasets=json.dumps([\"clinical\"]),\n",
    "    clinical_preprocess_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    clinical_preprocess_outputs=json.dumps([\"clinical-preprocessed\"]),\n",
    "    clinical_preprocess_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    \n",
    "    #Image preprocess\n",
    "    image_preprocess_script=\"python img-pre-processing.py\",\n",
    "    image_preprocess_datasets=json.dumps([\"images\"]),\n",
    "    image_preprocess_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    image_preprocess_outputs=json.dumps([\"images-preprocessed\"]),\n",
    "    image_preprocess_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    \n",
    "    #Clinical split\n",
    "    clinical_split_script=\"python split.py --datatype clinical\",\n",
    "    clinical_split_datasets=json.dumps([\"clinical-preprocessed\"]),\n",
    "    clinical_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    clinical_split_outputs=json.dumps([\"clinical-train\", \"clinical-test\", \"clinical-val\"]),\n",
    "    clinical_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"]),\n",
    "    \n",
    "    #Image split\n",
    "    image_split_script=\"python split.py --datatype image\",\n",
    "    image_split_datasets=json.dumps([\"images-preprocessed\"]),\n",
    "    image_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    image_split_outputs=json.dumps([\"images-train\", \"images-test\", \"images-val\"]),\n",
    "    image_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"])\t,\n",
    "    \n",
    "    #RNA split\n",
    "    rna_split_script=\"python split.py --datatype rna\",\n",
    "    rna_split_datasets=json.dumps([\"rna\"]),\n",
    "    rna_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    rna_split_outputs=json.dumps([\"rna-train\", \"rna-test\", \"rna-val\"]),\n",
    "    rna_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"]),\n",
    "    \n",
    "    #Training\n",
    "    job_group = 'default',\n",
    "    #Framework. One of tensorflow, pytorch, sklearn\n",
    "    framework = \"tensorflow\",\n",
    "    #Framework version\n",
    "    version = \"1.14\",\n",
    "    #In notebook DKUBE_USER_ACCESS_TOKEN is automatically picked up from env variable\n",
    "    #By default tf v1.14 image is used here, v1.13 or v1.14 can be used. \n",
    "    #Or any other custom image name can be supplied.\n",
    "    #For custom private images, please input username/password\n",
    "    training_container=json.dumps({'image':'docker.io/ocdr/d3-datascience-tf-cpu:v1.14', 'username':'', 'password': ''}),\n",
    "    #Name of the workspace in dkube. Update accordingly if different name is used while creating a workspace in dkube.\n",
    "    training_program=\"regression\",\n",
    "    #Script to run inside the training container    \n",
    "    training_script=\"python train_nn.py --epochs 5\",\n",
    "    #Input datasets for training. Update accordingly if different name is used while creating dataset in dkube.    \n",
    "    training_datasets=json.dumps([\"clinical-train\", \"clinical-val\", \"images-train\",\n",
    "                                  \"images-val\", \"rna-train\", \"rna-val\"]),\n",
    "    training_input_dataset_mounts=json.dumps([\"/opt/dkube/inputs/train/clinical\", \"/opt/dkube/inputs/val/clinical\",\n",
    "                                      \"/opt/dkube/inputs/train/images\", \"/opt/dkube/inputs/val/images\",\n",
    "                                      \"/opt/dkube/inputs/train/rna\", \"/opt/dkube/inputs/val/rna\"]),\n",
    "    training_outputs=json.dumps([\"regression-model\"]),\n",
    "    training_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    #Request gpus as needed. Val 0 means no gpu, then training_container=docker.io/ocdr/dkube-datascience-tf-cpu:v1.12    \n",
    "    training_gpus=0,\n",
    "    #Any envs to be passed to the training program    \n",
    "    training_envs=json.dumps([{\"steps\": 100}]),\n",
    "    \n",
    "    tuning=json.dumps({}),\n",
    "    \n",
    "    #Evaluation\n",
    "    evaluation_script=\"python evaluate.py\",\n",
    "    evaluation_datasets=json.dumps([\"clinical-test\", \"images-test\", \"rna-test\"]),\n",
    "    evaluation_input_dataset_mounts=json.dumps([\"/opt/dkube/inputs/test/clinical\", \"/opt/dkube/inputs/test/images\",\n",
    "                                      \"/opt/dkube/inputs/test/rna\"]),\n",
    "    evaluation_models=json.dumps([\"regression-model\"]),\n",
    "    evaluation_input_model_mounts=json.dumps([\"/opt/dkube/inputs/model\"]),\n",
    "    \n",
    "    #Serving\n",
    "    #Device to be used for serving - dkube mnist example trained on gpu needs gpu for serving else set this param to 'cpu'\n",
    "    serving_device='cpu',\n",
    "    #Serving image\n",
    "    serving_image=json.dumps({'image':'ocdr/tensorflowserver:1.14', 'username':'', 'password': ''}),\n",
    "    #Transformer image\n",
    "    transformer_image=json.dumps({'image':'docker.io/ocdr/d3-datascience-tf-cpu:v1.14', 'username':'', 'password': ''}),\n",
    "    #Script to execute the transformer\n",
    "    transformer_code=\"reg_demo/transformer.py\"):\n",
    "    \n",
    "    create_resource = _component('setup', 'reg-setup')(access_url,\n",
    "                                               auth_token,\n",
    "                                               user)\n",
    "    \n",
    "    clinical_preprocess = _component('preprocess', 'clinical-preprocess')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=clinical_preprocess_script,\n",
    "                                      datasets=clinical_preprocess_datasets, outputs=clinical_preprocess_outputs,\n",
    "                                      input_dataset_mounts=clinical_preprocess_input_mounts, output_mounts=clinical_preprocess_output_mounts).after(create_resource)\n",
    "    image_preprocess  = _component('preprocess', 'images-preprocess')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=image_preprocess_script,\n",
    "                                      datasets=image_preprocess_datasets, outputs=image_preprocess_outputs,\n",
    "                                      input_dataset_mounts=image_preprocess_input_mounts, output_mounts=image_preprocess_output_mounts).after(create_resource)\n",
    "                                      \n",
    "    clinical_split  = _component('preprocess', 'clinical-split')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=clinical_split_script,\n",
    "                                      datasets=clinical_split_datasets, outputs=clinical_split_outputs,\n",
    "                                      input_dataset_mounts=clinical_split_input_mounts,\n",
    "                                      output_mounts=clinical_split_output_mounts).after(clinical_preprocess)\n",
    "                                      \n",
    "    image_split  = _component('preprocess', 'images-split')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=image_split_script,\n",
    "                                      datasets=image_split_datasets, outputs=image_split_outputs,\n",
    "                                      input_dataset_mounts=image_split_input_mounts,\n",
    "                                      output_mounts=image_split_output_mounts).after(image_preprocess)\n",
    "                                      \n",
    "    rna_split  = _component('preprocess', 'rna-split')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=rna_split_script,\n",
    "                                      datasets=rna_split_datasets, outputs=rna_split_outputs,\n",
    "                                      input_dataset_mounts=rna_split_input_mounts, output_mounts=rna_split_output_mounts).after(create_resource)\n",
    "                                      \n",
    "    train       = _component('training', 'regression-model-training')(auth_token, training_container,\n",
    "                                    program=training_program, run_script=training_script,\n",
    "                                    datasets=training_datasets, outputs=training_outputs,\n",
    "                                    input_dataset_mounts=training_input_dataset_mounts,\n",
    "                                    output_mounts=training_output_mounts,\n",
    "                                    ngpus=training_gpus,\n",
    "                                    envs=training_envs,\n",
    "                                    tuning=tuning, job_group=job_group,\n",
    "                                    framework=framework, version=version).after(clinical_split).after(image_split).after(rna_split)\n",
    "    serving     = _component('serving', 'model-serving')(auth_token, train.outputs['artifact'], device=serving_device,\n",
    "                                serving_image=serving_image,\n",
    "                                transformer_image=transformer_image,\n",
    "                                transformer_project=training_program,\n",
    "                                transformer_code=transformer_code).after(train)\n",
    "    inference   = _component('viewer', 'model-inference')(auth_token, serving.outputs['servingurl'],\n",
    "                                 'regression', viewtype='inference').after(serving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"0\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(d3pipeline, 'dkube_regression_pl_full.tar.gz')\n",
    "# Upload this generated tarball into the Pipelines UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
