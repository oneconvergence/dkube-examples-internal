{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please wait till this cell completes and then run next cells. This just need to be run once per active kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env RELEASE_VERSION=1.0.0\n",
    "!pip install https://storage.googleapis.com/ml-pipeline/release/${RELEASE_VERSION}/kfp.tar.gz --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e MNIST Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "dkube_preprocessing_op      = components.load_component_from_file(\"../components/preprocess/component.yaml\")\n",
    "dkube_training_op           = components.load_component_from_file(\"../components/training/component.yaml\")\n",
    "dkube_serving_op            = components.load_component_from_file(\"../components/serving/component.yaml\")\n",
    "dkube_viewer_op             = components.load_component_from_file('../components/viewer/component.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='mnist',\n",
    "    description='sample mnist digits pipeline with dkube components'\n",
    ")\n",
    "def d3pipeline(\n",
    "    #Authentication token\n",
    "    auth_token,\n",
    "    #Name of the project in dkube\n",
    "    training_program,\n",
    "    #Dataset for preprocessing\n",
    "    preprocessing_dataset,\n",
    "    #Featureset for training\n",
    "    training_featureset,\n",
    "    #Output model \n",
    "    training_output_model,\n",
    "    #Dkube authentication token\n",
    "    data_preprocess_script = \"python featureset.py\",\n",
    "    data_preprocess_input_mounts = \"/opt/dkube/input\",\n",
    "    data_preprocess_output_mounts = \"/opt/dkube/output\",\n",
    "    #By default 'default' is used as the job group for runs\n",
    "    job_group = 'default',\n",
    "    #Framework. One of tensorflow, pytorch, sklearn\n",
    "    framework = \"tensorflow\",\n",
    "    #Framework version\n",
    "    version = \"1.14\",\n",
    "    #By default tf v1.14 image is used here, v1.13 or v1.14 can be used.\n",
    "    #Or any other custom image name can be supplied.\n",
    "    #For custom private images, please input username/password\n",
    "    training_container=json.dumps({'image':'docker.io/ocdr/d3-datascience-tf-cpu:fs-v1.14', 'username':'', 'password': ''}),\n",
    "    #Script to run inside the training container\n",
    "    training_script=\"python model.py\",\n",
    "    #Input dataset mount path\n",
    "    training_input_featureset_mount=\"/opt/dkube/input\",\n",
    "    #Output dataset mount paths\n",
    "    training_output_mount=\"/opt/dkube/output\",\n",
    "    #Request gpus as needed. Val 0 means no gpu, then training_container=docker.io/ocdr/dkube-datascience-tf-cpu:v1.12\n",
    "    training_gpus=0,\n",
    "    #Any envs to be passed to the training program\n",
    "    training_envs=json.dumps([{\"steps\": 100}]),\n",
    "    #Hyperparameter tuning info\n",
    "    tuning=json.dumps({}),\n",
    "    #Device to be used for serving - dkube mnist example trained on gpu needs gpu for serving else set this param to 'cpu'\n",
    "    serving_device='cpu',\n",
    "    #Serving image\n",
    "    serving_image=json.dumps({'image':'ocdr/tensorflowserver:1.14', 'username':'', 'password': ''}),\n",
    "    #Transformer image\n",
    "    transformer_image=json.dumps({'image':'docker.io/ocdr/d3-datascience-tf-cpu:v1.14', 'username':'', 'password': ''}),\n",
    "    #Script to execute the transformer\n",
    "    transformer_code=\"tensorflow/classification/mnist-fs/digits/transformer/transformer.py\"):\n",
    "    \n",
    "    preprocess = dkube_preprocessing_op(auth_token, training_container,\n",
    "                                      program=training_program, run_script=data_preprocess_script,\n",
    "                                      datasets=json.dumps([str(preprocessing_dataset)]), output_featuresets=json.dumps([str(training_featureset)]),\n",
    "                                      input_dataset_mounts=json.dumps([str(data_preprocess_input_mounts)]), output_featureset_mounts=json.dumps([str(data_preprocess_output_mounts)]))\n",
    "\n",
    "    train      = dkube_training_op(auth_token, training_container,\n",
    "                                    program=training_program, run_script=training_script,\n",
    "                                    featuresets=json.dumps([str(training_featureset)]), outputs=json.dumps([str(training_output_model)]),\n",
    "                                    input_featureset_mounts=json.dumps([str(training_input_featureset_mount)]),\n",
    "                                    output_mounts=json.dumps([str(training_output_mount)]),\n",
    "                                    ngpus=training_gpus, envs=training_envs,\n",
    "                                    tuning=tuning, job_group=job_group,\n",
    "                                    framework=framework, version=version).after(preprocess)\n",
    "    serving    = dkube_serving_op(auth_token, train.outputs['artifact'],\n",
    "                                device=serving_device, serving_image=serving_image,\n",
    "                                transformer_image=transformer_image,\n",
    "                                transformer_project=training_program,\n",
    "                                transformer_code=transformer_code).after(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(d3pipeline, 'dkube_mnist_fs_pl.tar.gz')\n",
    "# Upload this generated tarball into the Pipelines UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
