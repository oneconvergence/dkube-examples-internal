{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set appropriate sys path and import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client\n",
    "from kfp import components\n",
    "import json\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List existing pipeline experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dkube Dental experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_taxi_experiment = client.create_experiment(name='Chicao Taxi Cab pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e Dental Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "\n",
    "def dataflow_tf_data_validation_op(inference_data, validation_data,\n",
    "                                   column_names, key_columns, project, mode,\n",
    "                                   validation_output, volume,\n",
    "                                   step_name='validation'):\n",
    "    return dsl.ContainerOp(\n",
    "        name=step_name,\n",
    "        image='gcr.io/ml-pipeline/ml-pipeline-dataflow-tfdv:6ad2601ec7d04e842c212c50d5c78e548e12ddea',\n",
    "        arguments=[\n",
    "            '--csv-data-for-inference', inference_data,\n",
    "            '--csv-data-to-validate', validation_data,\n",
    "            '--column-names', column_names,\n",
    "            '--key-columns', key_columns,\n",
    "            '--project', project,\n",
    "            '--mode', mode,\n",
    "            '--output', '%s/{{workflow.name}}/validation' % validation_output,\n",
    "        ],\n",
    "        file_outputs={\n",
    "            'schema': '/schema.txt',\n",
    "            'validation': '/output_validation_result.txt',\n",
    "        },\n",
    "        pvolumes={validation_output: volume}\n",
    "    )\n",
    "\n",
    "\n",
    "def dataflow_tf_transform_op(train_data, evaluation_data, schema,\n",
    "                             project, preprocess_mode, preprocess_module,\n",
    "                             transform_output, volume,\n",
    "                             step_name='preprocess'):\n",
    "    return dsl.ContainerOp(\n",
    "        name=step_name,\n",
    "        image='gcr.io/ml-pipeline/ml-pipeline-dataflow-tft:6ad2601ec7d04e842c212c50d5c78e548e12ddea',\n",
    "        arguments=[\n",
    "            '--train', train_data,\n",
    "            '--eval', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--project', project,\n",
    "            '--mode', preprocess_mode,\n",
    "            '--preprocessing-module', preprocess_module,\n",
    "            '--output', '%s/{{workflow.name}}/transformed' % transform_output,\n",
    "        ],\n",
    "        file_outputs={'transformed': '/output.txt'},\n",
    "        pvolumes={transform_output: volume}\n",
    "    )\n",
    "\n",
    "\n",
    "def tf_train_op(transformed_data_dir, schema, learning_rate: float,\n",
    "                hidden_layer_size: int, steps: int, target: str,\n",
    "                preprocess_module, training_output, volume,\n",
    "                step_name='training'):\n",
    "    return dsl.ContainerOp(\n",
    "        name=step_name,\n",
    "        image='gcr.io/ml-pipeline/ml-pipeline-kubeflow-tf-trainer:5df2cdc1ed145320204e8bc73b59cdbd7b3da28f',\n",
    "        arguments=[\n",
    "            '--transformed-data-dir', transformed_data_dir,\n",
    "            '--schema', schema,\n",
    "            '--learning-rate', learning_rate,\n",
    "            '--hidden-layer-size', hidden_layer_size,\n",
    "            '--steps', steps,\n",
    "            '--target', target,\n",
    "            '--preprocessing-module', preprocess_module,\n",
    "            '--job-dir', '%s/{{workflow.name}}/train' % training_output,\n",
    "        ],\n",
    "        file_outputs={'train': '/output.txt'},\n",
    "        pvolumes={training_output: volume}\n",
    "    )\n",
    "\n",
    "\n",
    "def dataflow_tf_model_analyze_op(model: 'TensorFlow model', evaluation_data,\n",
    "                                 schema, project, analyze_mode,\n",
    "                                 analyze_slice_column, analysis_output,\n",
    "                                 volume, step_name='analysis'):\n",
    "    return dsl.ContainerOp(\n",
    "        name=step_name,\n",
    "        image='gcr.io/ml-pipeline/ml-pipeline-dataflow-tfma:6ad2601ec7d04e842c212c50d5c78e548e12ddea',\n",
    "        arguments=[\n",
    "            '--model', model,\n",
    "            '--eval', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--project', project,\n",
    "            '--mode', analyze_mode,\n",
    "            '--slice-columns', analyze_slice_column,\n",
    "            '--output', '%s/{{workflow.name}}/analysis' % analysis_output,\n",
    "        ],\n",
    "        file_outputs={'analysis': '/output.txt'},\n",
    "        pvolumes={analysis_output: volume}\n",
    "    )\n",
    "\n",
    "\n",
    "def dataflow_tf_predict_op(evaluation_data, schema, target: str,\n",
    "                           model: 'TensorFlow model', predict_mode, project,\n",
    "                           prediction_output, volume,\n",
    "                           step_name='prediction'):\n",
    "    return dsl.ContainerOp(\n",
    "        name=step_name,\n",
    "        image='gcr.io/ml-pipeline/ml-pipeline-dataflow-tf-predict:6ad2601ec7d04e842c212c50d5c78e548e12ddea',\n",
    "        arguments=[\n",
    "            '--data', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--target', target,\n",
    "            '--model', model,\n",
    "            '--mode', predict_mode,\n",
    "            '--project', project,\n",
    "            '--output', '%s/{{workflow.name}}/predict' % prediction_output,\n",
    "        ],\n",
    "        file_outputs={'prediction': '/output.txt'},\n",
    "        pvolumes={prediction_output: volume}\n",
    "    )\n",
    "\n",
    "\n",
    "def confusion_matrix_op(predictions, output, volume,\n",
    "                        step_name='confusion_matrix'):\n",
    "    return dsl.ContainerOp(\n",
    "        name=step_name,\n",
    "        image='gcr.io/ml-pipeline/ml-pipeline-local-confusion-matrix:5df2cdc1ed145320204e8bc73b59cdbd7b3da28f',\n",
    "        arguments=[\n",
    "            '--output', '%s/{{workflow.name}}/confusionmatrix' % output,\n",
    "            '--predictions', predictions,\n",
    "            '--target_lambda', \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\",\n",
    "        ],\n",
    "        pvolumes={output: volume}\n",
    "    )\n",
    "\n",
    "\n",
    "def roc_op(predictions, output, volume, step_name='roc'):\n",
    "    return dsl.ContainerOp(\n",
    "        name=step_name,\n",
    "        image='gcr.io/ml-pipeline/ml-pipeline-local-roc:5df2cdc1ed145320204e8bc73b59cdbd7b3da28f',\n",
    "        arguments=[\n",
    "            '--output', '%s/{{workflow.name}}/roc' % output,\n",
    "            '--predictions', predictions,\n",
    "            '--target_lambda', \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\",\n",
    "        ],\n",
    "        pvolumes={output: volume}\n",
    "    )\n",
    "\n",
    "\n",
    "def kubeflow_deploy_op(model: 'TensorFlow model', tf_server_name, pvc_name,\n",
    "                       pvolumes, step_name='deploy'):\n",
    "    return dsl.ContainerOp(\n",
    "        name=step_name,\n",
    "        image='gcr.io/ml-pipeline/ml-pipeline-kubeflow-deployer:727c48c690c081b505c1f0979d11930bf1ef07c0',\n",
    "        arguments=[\n",
    "            '--cluster-name', 'tfx-taxi-pipeline-on-prem',\n",
    "            '--model-export-path', model,\n",
    "            '--server-name', tf_server_name,\n",
    "            '--pvc-name', pvc_name,\n",
    "        ],\n",
    "        pvolumes=pvolumes\n",
    "    )\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Taxi Cab on-prem',\n",
    "    description='Example pipeline that does classification with model analysis based on a public BigQuery dataset for on-prem cluster.'\n",
    ")\n",
    "def taxi_cab_classification(\n",
    "        pvc_size='1Gi',\n",
    "        project='tfx-taxi-pipeline-on-prem',\n",
    "        column_names='taxi-cab-classification/column-names.json',\n",
    "        key_columns='trip_start_timestamp',\n",
    "        train='taxi-cab-classification/train.csv',\n",
    "        evaluation='taxi-cab-classification/eval.csv',\n",
    "        mode='local',\n",
    "        preprocess_module='taxi-cab-classification/preprocessing.py',\n",
    "        learning_rate=0.1,\n",
    "        hidden_layer_size=1500,\n",
    "        steps=3000,\n",
    "        analyze_slice_column='trip_start_hour'):\n",
    "\n",
    "    tf_server_name = 'taxi-cab-classification-model-{{workflow.name}}'\n",
    "\n",
    "    vop = dsl.VolumeOp(\n",
    "        name='create-volume',\n",
    "        resource_name='taxi-cab-data',\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=pvc_size\n",
    "    )\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(\n",
    "        '/mnt/%s' % train,\n",
    "        '/mnt/%s' % evaluation,\n",
    "        '/mnt/%s' % column_names,\n",
    "        key_columns,\n",
    "        project,\n",
    "        mode,\n",
    "        '/mnt',\n",
    "        vop.volume\n",
    "    )\n",
    "\n",
    "    preprocess = dataflow_tf_transform_op(\n",
    "        '/mnt/%s' % train,\n",
    "        '/mnt/%s' % evaluation,\n",
    "        validation.outputs['schema'],\n",
    "        project, mode,\n",
    "        '/mnt/%s' % preprocess_module,\n",
    "        '/mnt',\n",
    "        vop.volume\n",
    "    )\n",
    "\n",
    "    training = tf_train_op(\n",
    "        preprocess.output,\n",
    "        validation.outputs['schema'],\n",
    "        learning_rate,\n",
    "        hidden_layer_size,\n",
    "        steps,\n",
    "        'tips',\n",
    "        '/mnt/%s' % preprocess_module,\n",
    "        '/mnt',\n",
    "        vop.volume\n",
    "    )\n",
    "\n",
    "    analysis = dataflow_tf_model_analyze_op(\n",
    "        training.output,\n",
    "        '/mnt/%s' % evaluation,\n",
    "        validation.outputs['schema'],\n",
    "        project,\n",
    "        mode,\n",
    "        analyze_slice_column,\n",
    "        '/mnt',\n",
    "        vop.volume\n",
    "    )\n",
    "\n",
    "    prediction = dataflow_tf_predict_op(\n",
    "        '/mnt/%s' % evaluation,\n",
    "        validation.outputs['schema'],\n",
    "        'tips',\n",
    "        training.output,\n",
    "        mode,\n",
    "        project,\n",
    "        '/mnt',\n",
    "        vop.volume\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix_op(\n",
    "        prediction.output,\n",
    "        '/mnt',\n",
    "        vop.volume\n",
    "    )\n",
    "\n",
    "    roc = roc_op(\n",
    "        prediction.output,\n",
    "        '/mnt',\n",
    "        vop.volume\n",
    "    )\n",
    "\n",
    "    deploy = kubeflow_deploy_op(\n",
    "        training.output,\n",
    "        tf_server_name,\n",
    "        vop.output,\n",
    "        {'/mnt': vop.volume}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(taxi_cab_classification, 'chicago-taxi-cab.tar.gz')\n",
    "# Upload this generated tarball into the Pipelines UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Click the dkube-training stage to see the enhanced Dkube Datascience dashboard, metrics and graphs. Click the dkube-viewer stage for the simple UI to test the model predecitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(chicago_taxi_experiment.id, 'chicago-pipeline', 'chicago-taxi-cab.tar.gz', params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
