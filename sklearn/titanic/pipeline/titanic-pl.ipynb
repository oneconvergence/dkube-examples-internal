{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please wait till this cell completes and then run next cells. This just need to be run once per active kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env RELEASE_VERSION=1.0.0\n",
    "!pip install https://storage.googleapis.com/ml-pipeline/release/${RELEASE_VERSION}/kfp.tar.gz --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e MNIST Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "dkube_preprocessing_op      = components.load_component_from_file(\"../components/preprocess/component.yaml\")\n",
    "dkube_training_op           = components.load_component_from_file(\"../components/training/component.yaml\")\n",
    "dkube_serving_op            = components.load_component_from_file(\"../components/serving/component.yaml\")\n",
    "dkube_viewer_op             = components.load_component_from_file('../components/viewer/component.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='titanic',\n",
    "    description='Titanic pipeline'\n",
    ")\n",
    "def d3pipeline(\n",
    "\n",
    "    #Name of the project in dkube\n",
    "    training_program,\n",
    "    #Dataset for preprocessing\n",
    "    preprocessing_dataset,\n",
    "    #Featureset for training\n",
    "    training_featureset,\n",
    "    #Output model \n",
    "    training_output_model,\n",
    "    #Dkube authentication token\n",
    "    auth_token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\"),\n",
    "    \n",
    "    data_preprocess_script = \"python featureset.py --fs titanic-fs\",\n",
    "    data_preprocess_input_mounts = \"/opt/dkube/input\",\n",
    "    data_preprocess_output_mounts = \"/opt/dkube/output\",\n",
    "    #Framework. One of tensorflow, pytorch, sklearn\n",
    "    framework = \"sklearn\",\n",
    "    #Framework version\n",
    "    version = \"0.23.2\",\n",
    "    #By default tf v1.14 image is used here, v1.13 or v1.14 can be used.\n",
    "    #Or any other custom image name can be supplied.\n",
    "    #For custom private images, please input username/password\n",
    "    training_container=json.dumps({'image':'docker.io/ocdr/d3-datascience-tf-cpu:fs-v1.14', 'username':'', 'password': ''}),\n",
    "    #Script to run inside the training container\n",
    "    training_script=\"python model.py\",\n",
    "    #Input dataset mount path\n",
    "    training_input_featureset_mount=\"/opt/dkube/input\",\n",
    "    #Output dataset mount paths\n",
    "    training_output_mount=\"/opt/dkube/output\",\n",
    "    #Request gpus as needed. Val 0 means no gpu, then training_container=docker.io/ocdr/dkube-datascience-tf-cpu:v1.12\n",
    "    training_gpus=0,\n",
    "    #Any envs to be passed to the training program\n",
    "    #Hyperparameter tuning info\n",
    "    #Device to be used for serving - dkube mnist example trained on gpu needs gpu for serving else set this param to 'cpu'\n",
    "    serving_device='cpu',\n",
    "    #Serving image\n",
    "    serving_image=json.dumps({'image':'ocdr/sklearnserver:0.23.2', 'username':'', 'password': ''}),\n",
    "    #Transformer image\n",
    "    transformer_image=json.dumps({'image':'docker.io/ocdr/d3-datascience-tf-cpu:v1.14', 'username':'', 'password': ''}),\n",
    "    #Script to execute the transformer\n",
    "    transformer_code=\"sklearn/titanic/program/transformer.py\"):\n",
    "    \n",
    "    preprocess = dkube_preprocessing_op(auth_token, training_container,\n",
    "                                      program=training_program, run_script=data_preprocess_script,\n",
    "                                      datasets=json.dumps([str(preprocessing_dataset)]), output_featuresets=json.dumps([str(training_featureset)]),\n",
    "                                      input_dataset_mounts=json.dumps([str(data_preprocess_input_mounts)]), output_featureset_mounts=json.dumps([str(data_preprocess_output_mounts)]))\n",
    "\n",
    "    train      = dkube_training_op(auth_token, training_container,\n",
    "                                    program=training_program, run_script=training_script,\n",
    "                                    featuresets=json.dumps([str(training_featureset)]), outputs=json.dumps([str(training_output_model)]),\n",
    "                                    input_featureset_mounts=json.dumps([str(training_input_featureset_mount)]),\n",
    "                                    output_mounts=json.dumps([str(training_output_mount)]),\n",
    "                                    ngpus=training_gpus,\n",
    "                                    framework=framework, version=version).after(preprocess)\n",
    "    serving    = dkube_serving_op(auth_token, train.outputs['artifact'],\n",
    "                                device=serving_device, serving_image=serving_image,\n",
    "                                transformer_image=transformer_image,\n",
    "                                transformer_project=training_program,\n",
    "                                transformer_code=transformer_code).after(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(d3pipeline, 'dkube_titanic_pl.tar.gz')\n",
    "# Upload this generated tarball into the Pipelines UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")\n",
    "client = kfp.Client(existing_token=existing_token)\n",
    "try:\n",
    "  client.upload_pipeline(pipeline_package_path = 'dkube_titanic_pl.tar.gz', pipeline_name = 'Titanic with Featuresets', description = None)\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create titanic experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_experiments()\n",
    "# Create a new experiment\n",
    "try:\n",
    "    mnist_experiment = client.create_experiment(name='Dkube - Titanic')\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"titanic\"\n",
    "dataset = \"titanic\"\n",
    "featureset = \"titanic-fs\"\n",
    "model = \"titanic\"\n",
    "\n",
    "try:\n",
    "    pipeline_id = client.get_pipeline_id('Titanic with Featuresets')\n",
    "    run = client.run_pipeline(mnist_experiment.id, 'titanic_fs_pl', None, pipeline_id=pipeline_id,\n",
    "                              params={\"auth_token\":existing_token, \"training_program\":code, \"preprocessing_dataset\":dataset, \n",
    "                                      \"training_featureset\":featureset, \"training_output_model\":model})\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
